{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[0.4836807  0.12904078]] [0.25208092]\n",
      "20 [[0.19073965 0.17976454]] [0.25647584]\n",
      "40 [[0.12526977 0.19734412]] [0.28636193]\n",
      "60 [[0.10713477 0.19999956]] [0.2957679]\n",
      "80 [[0.10203895 0.20018579]] [0.29869592]\n",
      "100 [[0.10058876 0.20009924]] [0.29960024]\n",
      "120 [[0.1001715  0.20003995]] [0.29987794]\n",
      "140 [[0.10005032 0.20001438]] [0.29996285]\n",
      "160 [[0.10001484 0.20000486]] [0.29998872]\n",
      "180 [[0.10000441 0.2000016 ]] [0.29999658]\n",
      "200 [[0.1000013  0.20000051]] [0.29999897]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "# 使用 NumPy 生成假数据(phony data), 总共 100 个点.\n",
    "x_data = np.float32(np.random.rand(2, 100)) # 随机输入\n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300\n",
    "\n",
    "# 构造一个线性模型\n",
    "# \n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "# 最小化方差\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "# 初始化变量\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "# 启动图 (graph)\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "# 拟合平面\n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print (step, sess.run(W), sess.run(b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 [[-0.4386059  0.7297468]] [0.7598344]\n",
      "20 [[-0.06513421  0.25576118]] [0.36253497]\n",
      "40 [[0.05812038 0.20348544]] [0.32149956]\n",
      "60 [[0.08862594 0.19794993]] [0.30742544]\n",
      "80 [[0.09669914 0.19861808]] [0.30257154]\n",
      "100 [[0.09898645 0.19938228]] [0.300892]\n",
      "120 [[0.0996751  0.19975707]] [0.30030972]\n",
      "140 [[0.09989268 0.19990976]] [0.3001076]\n",
      "160 [[0.09996385 0.19996744]] [0.30003738]\n",
      "180 [[0.09998766 0.19998842]] [0.300013]\n",
      "200 [[0.09999578 0.19999593]] [0.3000045]\n"
     ]
    }
   ],
   "source": [
    ">>> import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "x_data = np.float32(np.random.rand(2, 100)) \n",
    "y_data = np.dot([0.100, 0.200], x_data) + 0.300\n",
    "\n",
    "\n",
    "b = tf.Variable(tf.zeros([1]))\n",
    "W = tf.Variable(tf.random_uniform([1, 2], -1.0, 1.0))\n",
    "y = tf.matmul(W, x_data) + b\n",
    "\n",
    "loss = tf.reduce_mean(tf.square(y - y_data))\n",
    "optimizer = tf.train.GradientDescentOptimizer(0.5)\n",
    "train = optimizer.minimize(loss)\n",
    "\n",
    "init = tf.initialize_all_variables()\n",
    "\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "for step in range(0, 201):\n",
    "    sess.run(train)\n",
    "    if step % 20 == 0:\n",
    "        print (step, sess.run(W), sess.run(b))\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conjugate Gradient x:\n",
      "[[  50.]\n",
      " [  99.]\n",
      " [ 147.]\n",
      " [ 194.]\n",
      " [ 240.]\n",
      " [ 285.]\n",
      " [ 329.]\n",
      " [ 372.]\n",
      " [ 414.]\n",
      " [ 455.]\n",
      " [ 495.]\n",
      " [ 534.]\n",
      " [ 572.]\n",
      " [ 609.]\n",
      " [ 645.]\n",
      " [ 680.]\n",
      " [ 714.]\n",
      " [ 747.]\n",
      " [ 779.]\n",
      " [ 810.]\n",
      " [ 840.]\n",
      " [ 869.]\n",
      " [ 897.]\n",
      " [ 924.]\n",
      " [ 950.]\n",
      " [ 975.]\n",
      " [ 999.]\n",
      " [1022.]\n",
      " [1044.]\n",
      " [1065.]\n",
      " [1085.]\n",
      " [1104.]\n",
      " [1122.]\n",
      " [1139.]\n",
      " [1155.]\n",
      " [1170.]\n",
      " [1184.]\n",
      " [1197.]\n",
      " [1209.]\n",
      " [1220.]\n",
      " [1230.]\n",
      " [1239.]\n",
      " [1247.]\n",
      " [1254.]\n",
      " [1260.]\n",
      " [1265.]\n",
      " [1269.]\n",
      " [1272.]\n",
      " [1274.]\n",
      " [1275.]\n",
      " [1275.]\n",
      " [1274.]\n",
      " [1272.]\n",
      " [1269.]\n",
      " [1265.]\n",
      " [1260.]\n",
      " [1254.]\n",
      " [1247.]\n",
      " [1239.]\n",
      " [1230.]\n",
      " [1220.]\n",
      " [1209.]\n",
      " [1197.]\n",
      " [1184.]\n",
      " [1170.]\n",
      " [1155.]\n",
      " [1139.]\n",
      " [1122.]\n",
      " [1104.]\n",
      " [1085.]\n",
      " [1065.]\n",
      " [1044.]\n",
      " [1022.]\n",
      " [ 999.]\n",
      " [ 975.]\n",
      " [ 950.]\n",
      " [ 924.]\n",
      " [ 897.]\n",
      " [ 869.]\n",
      " [ 840.]\n",
      " [ 810.]\n",
      " [ 779.]\n",
      " [ 747.]\n",
      " [ 714.]\n",
      " [ 680.]\n",
      " [ 645.]\n",
      " [ 609.]\n",
      " [ 572.]\n",
      " [ 534.]\n",
      " [ 495.]\n",
      " [ 455.]\n",
      " [ 414.]\n",
      " [ 372.]\n",
      " [ 329.]\n",
      " [ 285.]\n",
      " [ 240.]\n",
      " [ 194.]\n",
      " [ 147.]\n",
      " [  99.]\n",
      " [  50.]]\n",
      "done Conjugate Gradient!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A = np.zeros((100, 100))\n",
    "for i in range(100): #generate A\n",
    "    for j in range(100):\n",
    "        if (i == j):\n",
    "            A[i, j] = 2\n",
    "        if (abs(i - j) == 1):\n",
    "            A[i, j] = A[j, i] = -1\n",
    "b = np.ones((100, 1))  #generate b\n",
    "print(\"Conjugate Gradient x:\")\n",
    "x=np.zeros((100,1)) # 初始值x0\n",
    " \n",
    "r=b-np.dot(A,x)\n",
    "p=r  #p0=r0\n",
    "#while np.linalg.norm(np.dot(A, x) - b) / np.linalg.norm(b) >= 10 ** -6:\n",
    "for i in range(100):\n",
    "    r1=r\n",
    "    a=np.dot(r.T,r)/np.dot(p.T,np.dot(A,p))\n",
    "    x = x + a * p    #x(k+1)=x(k)+a(k)*p(k)\n",
    "    r=b-np.dot(A,x)  #r(k+1)=b-A*x(k+1)\n",
    "    q = np.linalg.norm(np.dot(A, x) - b) / np.linalg.norm(b)\n",
    "    if q<10**-6:\n",
    "        break\n",
    "    else:\n",
    "        beta=np.linalg.norm(r)**2/np.linalg.norm(r1)**2\n",
    "        p=r+beta*p  #p(k+1)=r(k+1)+beta(k)*p(k)\n",
    " \n",
    "print(x)\n",
    "print(\"done Conjugate Gradient!\")\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
